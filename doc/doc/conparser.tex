\documentclass[12pt]{article}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{hyperref}
\usepackage{CJK}
\usepackage{times}
\makeindex
\title{Phrase-Structure Parsing}
\begin{document}
\begin{CJK}{GBK}{song}
\maketitle

\section{How to compile}
Suppose that ZPar has been downloaded to the directory \textit{zpar}. To make a phrase-structure parsing system for English, type \textit{make english.conparser}. This will create a directory \textit{zpar/dist/english.conparser}, in which there are two files: \textit{train} and \textit{conparser}. The file \textit{train} is used to train a parsing model,and the file \textit{conparser} is used to parse new texts using a trained parsing model. Similarly, we can make a phrase-structure parsing system for Chinese by typing \textit{make chinese.conparser}. The \textit{train} and \textit{conparser} files are created under the directory of \textit{zpar/dist/chinese.conparser}. Note that the English and Chinese parsers are designed specifically for \href{http://www.cis.upenn.edu/~treebank/}{Penn Treebanks}.
\section{Format of inputs and outputs}
The input file to the \textit{train} executable contains a set of parse trees, one for each line. An example parse tree is as follows:
\\
\\
%\begin{tabular}{l}
\hspace{3cm} ( S r ( NP r ( NNP t Ms. )  ( NNP t Haag )  )  ( S l* ( VP l ( VBZ t plays )  ( NP s ( NNP t Elianti )  )  )  ( . t . )  )  )
\\
\\
%\end{tabular}
The format is different from the original format used in Penn Treebanks. Here is a \href{con_files/binarize.py}{Python script} to convert the original Penn Treebank format to the ZPar format. The usage is
\\
\\
\hspace{3cm} python binarize.py $<$rule-file$>$ $<$input-file$>$ \\
\\
Here \textit{rule-file} is a file containing head-finding rules (see the \href{con_files/rule.txt}{example rules} for Penn Chinese Treebank), and the conversion results will be printed to the console. Note that, in the respect of Chinese, the encoding of input-file to \textit{binarize.py} should be \textit{gb} and the output will be encoded in \textit{utf8}. Here is a \href{seg_files/gb2utf.py}{script} that transfers files that are encoded in \textit{gb} to the \textit{utf8} encoding.
\\
\\
The input file to the \textit{conparser} contain POS tagged sentences. The formats for English and Chinese are different.
\\
\\
\hspace{2cm} \textbf{English}: \\
\hspace{3cm} Ms./NNP Haag/NNP plays/VBZ Elianti/NNP\\
\\
\hspace{2cm}\textbf{Chinese}: \\
\hspace{3cm} ZPar$\_$NR 可以$\_$MD 分析$\_$VV 中文$\_$NN 和$\_$CC 英文$\_$NN \\
\\
For Chinese, inputs to both \textit{train} and \textit{conparser} must be encoded in \textit{utf8}.
\section{How to train a model}
To train a model, use
\\
\\
\hspace{3cm} zpar/dist/english.conparser/train $<$train-file$>$ $<$model-file$>$ $<$number of iterations$>$ \\
\\
For example, using the \href{con_files/train.txt}{example train file}, you can train a  model by
\\
\\
\hspace{3cm} zpar/dist/english.conparser/train train.txt model 1 \\
\\
After training is completed, a new file \textit{model} will be created in the current directory, which can be used to parse POS-tagged sentences. The above command performs training with one iteration (see Section~\ref{tuning}) using the training file. The commands for training Chinese parsing models are the same.
\section{How to parse new texts}
To apply an existing model to parse new texts, use
\\
\\
\hspace{3cm} zpar/dist/english.conparser/conparser $<$input-file$>$ $<$output-file$>$  $<$model$>$
\\
\\
For example, using the model we just trained, we can parse \href{con_files/input.txt}{an example input} by
\\
\\
\hspace{3cm} zpar/dist/english.conparser/conparser input.txt output.txt model
\\
\\
The output file contains automatically parsed trees. The commands for parsing Chinese texts are the same.
\section{Outputs and evaluation}
In order to evaluate the quality of the outputs, we can manually specify the gold parse trees of a sample, and compare the outputs with the correct sample.
\\
\\
Manually specified parse trees of the input file are given in \href{con_files/reference.txt}{this example reference file}. Refer to \href{http://nlp.cs.nyu.edu/evalb/}{evalb} to obtain a software that performs automatic evaluation.
\\
\\
Using the above \textit{output.txt} and \textit{reference.txt}, we can evaluate the accuracies by typing
\\
\\
\hspace{3cm} ./evalb -p $<$config.file$>$ output.txt reference.txt
\\
\\
Here \textit{config.file} sets running parameters of the evaluation. \href{con_files/COLLINS.prm}{COLLINS.prm} is a widely used configuration file.
Evaluation results will be printed to the console.
\section{How to tune the performance of a system}
\label{tuning}
The performance of the system after one training iteration may not be optimal. You can try training a model for another few iterations, after each you compare the performance. You can choose the model that gives the highest f-score on your test data. We conventionally call this test file the development test data, because you develop a parsing model using this. Here is a \href{con_files/test.sh}{a shell script} that automatically trains the parser for 30 iterations, and after the $i$th iteration, stores the model file to model.$i$. You can compare the f-score of all 30 iterations and choose model.$k$, which gives the best f-score, as the final model. In this file, this is a variable called \textit{parser}. You need to set this variable to the relative directory of \textit{zpar/dist/english.conparsr} or \textit{zpar/dist/chinese.conparser}.
\section{Source code}
The source code for the English phrase-structure parser can be found at
\\
\\
\hspace{3cm} zpar/src/common/conparser/implementation/ENGLISH\_CONPARSER\_IMPL
\\
\\
where ENGLISH\_CONPARSER\_IMPL is a macro defined in \textit{Makefile}, and specifies a specific implementation for the English phrase-structure parser.
\\
\\
The source code for the Chinese phrase-structure parser can be found at
\\
\\
\hspace{3cm} zpar/src/common/conparser/implementation/CHINESE\_CONPARSER\_IMPL
\\
\\
where CHINESE\_CONPARSER\_IMPL is a macro defined in \textit{Makefile}, and specifies a specific implementation for the Chinese phrase-structure parser.
\begin{thebibliography}{99}
\bibitem{bib-1}
%Yue Zhang and Stephen Clark. 2011. Syntactic Processing Using the Generalized Perceptron and Beam Search. \textit{Computational Linguistics}, 37(1):105-151.
Yue Zhang and Stephen Clark. 2009. Transition-Based Parsing of the Chinese Treebank using a Global Discriminative Model. In {\em Proc. of IWPT}, pages 162-171.
\end{thebibliography}
\end{CJK}
\end{document}
